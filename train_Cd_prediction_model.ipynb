{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","mount_file_id":"12YZtrwyMwSB_VxsOXIq7nn-8bRSfG--c","authorship_tag":"ABX9TyM1M+bF0aX7Xw5GPq8LWlq5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Author : leekyoj@kaist.ac.kr\n","\n","# --- 1) Download data ---\n","!gdown --folder https://drive.google.com/drive/folders/1G_Nc1YgwjXEAV92XX2onVTOQmgWDrnZd?usp=sharing\n","!unzip processed_data/data_96.zip -d ./data_96"],"metadata":{"id":"wTZNKJG12yh4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import sys\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import time\n","import torch.optim as optim"],"metadata":{"id":"_ijnPvP3pi3T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 2) Data Loading ---\n","\n","SAVE_DIR = './data_96'\n","TRAINED_DIR = './trained_models'\n","OUTPUT_DIR = './output'\n","\n","for d in [SAVE_DIR, TRAINED_DIR, OUTPUT_DIR]:\n","    os.makedirs(d, exist_ok=True)\n","    print(f\"Created or exists: {d}\")\n","\n","def load_dataset(prefix):\n","    x1 = np.load(os.path.join(SAVE_DIR, f\"{prefix}_x1.npy\"), mmap_mode='r')\n","    x1 = np.array(x1, copy=True)\n","    x2 = np.load(os.path.join(SAVE_DIR, f\"{prefix}_x2.npy\"), mmap_mode='r')\n","    x2 = np.array(x2, copy=True)\n","    y  = np.load(os.path.join(SAVE_DIR, f\"{prefix}_y.npy\"),  mmap_mode='r')\n","    y = np.array(y, copy=True)\n","\n","    x1 = torch.from_numpy(x1).permute(0,3,1,2).float()\n","    x2 = torch.from_numpy(x2).permute(0,3,1,2).float()\n","    y  = torch.from_numpy(y).float()\n","    print(f\"Loaded {prefix}: x1 {x1.shape}, x2 {x2.shape}, y {y.shape}\")\n","    return x1, x2, y\n","\n","x_train1, x_train2, train_y = load_dataset('train')\n","x_val1,   x_val2,   val_y   = load_dataset('val')\n","x_test1,  x_test2,  test_y  = load_dataset('test')\n","\n","train_loader = DataLoader(TensorDataset(x_train1, x_train2, train_y),batch_size=1024,  sampler=RandomSampler(x_train1), pin_memory=True)\n","val_loader   = DataLoader(TensorDataset(x_val1,   x_val2,   val_y  ),batch_size=1024,  sampler=SequentialSampler(x_val1),pin_memory=True)\n","test_loader  = DataLoader(TensorDataset(x_test1,  x_test2,  test_y ),batch_size=1024,  sampler=SequentialSampler(x_test1), pin_memory=True)"],"metadata":{"id":"_xsGdq3CqHPG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 3) Check image and drag coefficient ---\n","import matplotlib.pyplot as plt\n","\n","def show_image_with_label(x, y, idx, title):\n","    img = x[idx].permute(1,2,0).numpy()   # (H, W, C)\n","    plt.figure(figsize=(4,4))\n","    plt.imshow(img)\n","    plt.axis('off')\n","    plt.title(f\"{title} #{idx}\\nLabel: {y[idx].item():.4f}\")\n","    plt.show()\n","\n","idx = 15\n","print(\"▶️ Train set:\")\n","show_image_with_label(x_train1, train_y, idx, \"Train Depth\")\n","show_image_with_label(x_train2, train_y, idx, \"Train Normal\")\n","\n","print(\"\\n▶️ Validation set:\")\n","show_image_with_label(x_val1, val_y, idx, \"Val Depth\")\n","show_image_with_label(x_val2, val_y, idx, \"Val Normal\")\n","\n","print(\"\\n▶️ Test set:\")\n","show_image_with_label(x_test1, test_y, idx, \"Test Depth\")\n","show_image_with_label(x_test2, test_y, idx, \"Test Normal\")"],"metadata":{"id":"j00MnCzzbwAo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 4) Define CNN model ---\n","class FusionNet_CNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.dropout_rate = 0.5\n","\n","        self.feature_extractor = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n","\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Dropout2d(self.dropout_rate),\n","\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Dropout2d(self.dropout_rate),\n","\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Dropout2d(self.dropout_rate),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","            nn.AdaptiveAvgPool2d(1),\n","            nn.Flatten()\n","        )\n","\n","        self.regressor = nn.Sequential(\n","            nn.Linear(512 * 2, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.4),\n","\n","            nn.Linear(512, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(256, 1)\n","        )\n","\n","    def forward(self, x1, x2):\n","        f1 = self.feature_extractor(x1)\n","        f2 = self.feature_extractor(x2)\n","\n","        combined = torch.cat([f1, f2], dim=1)\n","\n","        return self.regressor(combined)\n"],"metadata":{"id":"h_LDG3ipnj7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 5) Setup train hyperparameter ---\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"GPU device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n","model = FusionNet_CNN().to(device)\n","\n","from torchsummary import summary\n","summary(model, input_size=[(3, 96, 96), (3, 96, 96)])\n","\n","lr = 5e-3\n","epochs = 10\n","batch_size = 1024\n","\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","criterion = nn.MSELoss()\n"],"metadata":{"id":"1dCFlrvZn7c5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 6) Train and evaluation function ---\n","def train_one_epoch(loader):\n","    model.train()\n","    total_loss = 0.0\n","    start_time = time.time()\n","    for i, (x1, x2, y) in enumerate(loader, 1):\n","        x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n","\n","        optimizer.zero_grad()\n","        out = model(x1, x2)\n","        loss = criterion(out.squeeze(), y)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        if i % 5 == 0:\n","            print(f'  Batch {i}/{len(loader)}  loss={loss.item():.4f}', end='\\r')\n","\n","    elapsed = time.time() - start_time\n","    print(f' Epoch train time: {elapsed:.2f}s')\n","    return total_loss / len(loader)\n","\n","\n","def evaluate(loader):\n","    model.eval()\n","    total_loss = 0.0\n","    preds = []\n","    with torch.no_grad():\n","        for x1, x2, y in loader:\n","            x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n","\n","            out = model(x1, x2)\n","            loss = criterion(out.squeeze(), y)\n","\n","            total_loss += loss.item()\n","            preds.append(out.cpu().numpy())\n","\n","    return total_loss / len(loader), np.concatenate(preds)\n"],"metadata":{"id":"V47hWHdBoO05"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Training loop ---\n","best_val=float('inf')\n","save_path=os.path.join(TRAINED_DIR,'pure_cnn.pt')\n","print('\\n--- Starting Training ---')\n","for e in range(1,epochs+1):\n","    print(f'Epoch {e}/{epochs}')\n","    tr_loss = train_one_epoch(train_loader)\n","    val_loss, _ = evaluate(val_loader)\n","    print(f' - train={tr_loss:.4f}, val={val_loss:.4f}')\n","\n","    if val_loss < best_val:\n","        best_val = val_loss\n","        state_dict = model.state_dict()\n","        torch.save(state_dict, save_path)\n","        print(f\"  Improved, model saved to {save_path}\")\n","\n","print('--- Training Finished ---')"],"metadata":{"id":"htC3eJpO-AZH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('\\n--- Evaluating Test Set ---')\n","if os.path.exists(save_path):\n","    state = torch.load(save_path)\n","    model.load_state_dict(state)\n","else:\n","    print('Warning: no checkpoint')\n","\n","loss_test, preds = evaluate(test_loader)\n","costs = test_y.cpu().numpy()\n","predv = preds.squeeze()\n","r2 = float('nan')\n","try:\n","    r2 = np.corrcoef(costs, predv)[0,1]**2\n","except:\n","    pass\n","mse = float(((predv-costs)**2).mean())\n","print(f'Test MSE={mse:.4f}, R2={r2:.4f}')\n","\n","# Visualize of training result\n","plot = os.path.join(OUTPUT_DIR, f'improved_pure_cnn_R2_{r2:.4f}_MSE_{mse:.4f}.png')\n","plt.figure(figsize=(5,5))\n","plt.scatter(costs, predv, s=10)\n","mn, mx = costs.min(), costs.max()\n","plt.plot([mn, mx], [mn, mx], '--k')\n","plt.gca().set_aspect('equal')\n","plt.title('Ground Truth vs Prediction')\n","plt.xlabel('GT')\n","plt.ylabel('Pred')\n","plt.savefig(plot)\n","plt.show()\n","print(f'Plot saved to {plot}')\n","print('--- Done ---')\n"],"metadata":{"id":"LaEFVryWAuPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.models import resnet18\n","from torchvision.models.resnet import ResNet18_Weights\n","\n","class FusionNet_resnet(nn.Module):\n","    def __init__(self, pretrained=True):\n","        super().__init__()\n","        weights = ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n","\n","        # Backbone 1\n","        self.backbone1 = resnet18(weights=weights).to(device)\n","        num_ftrs1 = self.backbone1.fc.in_features\n","        self.backbone1.fc = nn.Identity()\n","\n","        # Backbone 2\n","        self.backbone2 = resnet18(weights=weights).to(device)\n","        num_ftrs2 = self.backbone2.fc.in_features\n","        self.backbone2.fc = nn.Identity()\n","\n","        # Regression head\n","        self.regressor = nn.Sequential(\n","            nn.Linear(num_ftrs1 + num_ftrs2, 256),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(256, 1)\n","        )\n","\n","    def forward(self, x1, x2):\n","        f1 = self.backbone1(x1)\n","        f2 = self.backbone2(x2)\n","        combined = torch.cat([f1, f2], dim=1)\n","        return self.regressor(combined)"],"metadata":{"id":"Mvfjam2KxLxv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 5) Setup train hyperparameter ---\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"GPU device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n","model = FusionNet_resnet().to(device)\n","\n","from torchsummary import summary\n","summary(model, input_size=[(3, 96, 96), (3, 96, 96)])\n","\n","lr = 5e-4\n","epochs = 10\n","batch_size = 1024\n","\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","criterion = nn.MSELoss()\n","\n","# --- Training loop ---\n","best_val=float('inf')\n","save_path=os.path.join(TRAINED_DIR,'resnet18.pt')\n","print('\\n--- Starting Training ---')\n","for e in range(1,epochs+1):\n","    print(f'Epoch {e}/{epochs}')\n","    tr_loss = train_one_epoch(train_loader)\n","    val_loss, _ = evaluate(val_loader)\n","    print(f' - train={tr_loss:.4f}, val={val_loss:.4f}')\n","\n","    if val_loss < best_val:\n","        best_val = val_loss\n","        state_dict = model.state_dict()\n","        torch.save(state_dict, save_path)\n","        print(f\"  Improved, model saved to {save_path}\")\n","\n","print('--- Training Finished ---')"],"metadata":{"id":"A6HIcHZKxr_4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('\\n--- Evaluating Test Set ---')\n","if os.path.exists(save_path):\n","    state = torch.load(save_path)\n","    model.load_state_dict(state)\n","else:\n","    print('Warning: no checkpoint')\n","\n","loss_test, preds = evaluate(test_loader)\n","costs = test_y.cpu().numpy()\n","predv = preds.squeeze()\n","r2 = float('nan')\n","try:\n","    r2 = np.corrcoef(costs, predv)[0,1]**2\n","except:\n","    pass\n","mse = float(((predv-costs)**2).mean())\n","print(f'Test MSE={mse:.4f}, R2={r2:.4f}')\n","\n","# Visualize of training result\n","plot = os.path.join(OUTPUT_DIR, f'resnet18_R2_{r2:.4f}_MSE_{mse:.4f}.png')\n","plt.figure(figsize=(5,5))\n","plt.scatter(costs, predv, s=10)\n","mn, mx = costs.min(), costs.max()\n","plt.plot([mn, mx], [mn, mx], '--k')\n","plt.gca().set_aspect('equal')\n","plt.title('Ground Truth vs Prediction')\n","plt.xlabel('GT')\n","plt.ylabel('Pred')\n","plt.savefig(plot)\n","plt.show()\n","print(f'Plot saved to {plot}')\n","print('--- Done ---')\n"],"metadata":{"id":"0tlF38Qk0fDj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inference pretrained-resnet\n","# https://colab.research.google.com/drive/1jWA2GOFCZIOH5SPooDTwAiig_-MYnWA9?usp=sharing"],"metadata":{"id":"rzC21MKhxNAE"},"execution_count":null,"outputs":[]}]}